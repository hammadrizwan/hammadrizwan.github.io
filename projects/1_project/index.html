<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Resolving Lexical Bias in Model Editing | Hammad Rizwan </title> <meta name="author" content="Hammad Rizwan"> <meta name="description" content="A weights preserving model editing approach using Projector Editor Network (PENME)."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/dal.png?390ec951b0786085b897c4cf9aaa42e2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hammadrizwan.github.io/projects/1_project/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hammad</span> Rizwan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Resolving Lexical Bias in Model Editing</h1> <p class="post-description">A weights preserving model editing approach using Projector Editor Network (PENME).</p> </header> <article> <h3 id="summary">Summary:</h3> <p>Model editing aims to modify the outputs of large language models after they are trained. Previous approaches have often involved direct alterations to model weights, which can result in model degradation. Recent techniques avoid making modifications to the model’s weights by using an adapter that applies edits to the model when triggered by semantic similarity in the representation space. We demonstrate that current adapter methods are critically vulnerable to strong lexical biases, leading to issues such as applying edits to irrelevant prompts with overlapping words. This paper presents a principled approach to learning a disentangled representation space that facilitates precise localization of edits by maintaining distance between irrelevant prompts while preserving proximity among paraphrases. In our empirical study, we show that our method (Projector Editor Networks for Model Editing - PENME) achieves state-of-the-art model editing results while being more computationally efficient during inference than previous methods and adaptable across different architectures. <a class="citation" href="#Rizwan25Editing">(Rizwan et al., 2025)</a></p> <h3 id="overview">Overview</h3> <p>Our research shows that performance of weight-preserving meth-ods is heavily reliant on scoping mechanism which suffers from a critical vulnerability of Lexical bias Figure 1, prompts with similar lexical tokens but different semantics that are closer together in the representation space compared to a prompt and its respective paraphrases. Lexical bias prevents current adapter-based methods from effectively being able to balance generalization to unseen paraphrases and “misfiring” on semantically dissimilar (irrelevant) prompts.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lexical_domince_visual_figure1-480.webp 480w,/assets/img/lexical_domince_visual_figure1-800.webp 800w,/assets/img/lexical_domince_visual_figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/lexical_domince_visual_figure1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. An illustration of lexical bias in embeddings: a) a low similarity threshold (illustrated with the circle) results in failing to edit paraphrases. b) A high similarity threshold results in misfires with irrelevant prompts. c) illustrates our solution which restructures the representation space. </div> <p>To examine the lexical bias of representations, we randomly sampled 500 entries from the Counterfact dataset. For each entry, we created triplets consisting of an edit prompt, a randomly sampled paraphrase prompt and an irrelevant prompt with \(\textbf{high lexical overlap}\). These triplets are fed into various models, and representation vectors (\(\vec{x_{i}},\vec{p_{i}},\vec{p^{\neg}_{i}}\)) from the feed-forward block of each layer \(l\) are extracted. We select either averaged token representations or dedicated sentence representations, based on whether a given model offers a specific token for sentence-level representation. We calculate two sets of pairwise Euclidean distances (1) Between edit representations and paraphrase representations \(\|\|\vec{x_{i}}-\vec{p_{i}}\|\|_2\) (2) Between edit representations and irrelevant prompts representations \(\|\|\vec{x_{i}}-\vec{p^{\neg}_{i}}\|\|_2\). We then compare these distances to determine if irrelevant prompts are closer to the edits than the paraphrases \(\|\|\vec{x_{i}} -\vec{p_{i}}\|\|_2 &gt; \|\|\vec{x_{i}} -\vec{p^{\neg}_{i}}\|\|_2\). Figure 2 displays the percentage of samples where irrelevant prompts \(\textit{were closer}\) to the edits.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lexical_dominance_models_full-480.webp 480w,/assets/img/lexical_dominance_models_full-800.webp 800w,/assets/img/lexical_dominance_models_full-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/lexical_dominance_models_full.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2.Percentage of samples where edits are closer to irrelevant prompts as compared to paraphrases in the representations space of different models across all layers. T5-small, GPT2-XL and Llama-2-7b have 6, 32, 48 layers, respectively. </div> <p>To resolve this issue we propose PENME, a model editing framework that learns a projection network that maps the model’s representation space to a new representation space where lexical bias is minimized. We integrate our projection network in an adapter-based retrieval scheme for model editing, demon- strating, for the first time in adapter-based approaches, high efficacy in both paraphrase execution (generalization) and prevention of misfires on irrelevant prompts (locality).</p> <p>PENME, illustrated in Figure 3, consists of two components: (1) \(\textbf{Projection Network (g)}\) projects model activations denoted $h_l(input)$ at layer $l$ into a distinct representation space $g(h_l(input))$. (2) \textbf{Key-Value Codebook} stores the projected model activations $g(h_l(input))$ at layer $l$ as keys and corresponding values containing a learned similarity threshold ($\delta$) and the new associated output information $y_i$. This paper only considers storing strings as \(y_i\), but vectors or LoRA block indices can also be stored as values, which facilitate playback approaches.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PENME-480.webp 480w,/assets/img/PENME-800.webp 800w,/assets/img/PENME-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/PENME.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. PENME uses a projection network that interfaces with the pointwise feed-forward layer output in a transformer block. The projection network, coupled with key-value codebook storage, acts as a scoping mechanism by comparing projection outputs with codebook entries. This determines whether the current input relates to a specific edit or should pass through the model unmodified. </div> <h3 id="projector-training">Projector Training</h3> <p>The project consists of two layer neural network with non-linearity in between much like pointwise feed forward layer in the transformer mode. Our training loss is inspired by contrastive learning and is defined by the following loss function:</p> \[\begin{aligned} \mathcal{L}(\vec{x_i}, \vec{z}) &amp;= (1-t)\,\tfrac{1}{2}\lVert \vec{x_i} - \vec{z} \rVert_2^2 \\ &amp;\quad + t \,\tfrac{1}{2} \big[\max(0, m - \lVert \vec{x_i} - \vec{z} \rVert_2)\big]^2, \\ t &amp;= \begin{cases} 1, &amp; \text{if } \vec{z} \gets \vec{p_{ij}}, \\ 0, &amp; \text{if } \vec{z} \gets \vec{p^{\neg}_{ij}} \lor \vec{x_l}. \end{cases} \end{aligned}\] <p>where $t$ is the target ${0,1}$ which is 0 when the training pair is \(\{x_i,p_{ij}\}\) (edit, paraphrase) and 1 when the training pair is \(\{x_i,p^{\neg}_{ij}\}\) (edit, irrelevant) or the inter-edit (or edit-to-edit) pair \(\{x_i,x_l\}\) where we sample an unrelated edit, \(m\) is the margin which pushes \(\vec{p^{\neg}_{ij}}\) at least $m$ distance away from \(\vec{x_{i}}\). The projection network is trained such that for all samples in a dataset, edits $x_i$ and edit paraphrases \(p_{ij}\) are close together while edits \(x_i\) and irrelevant $p^{\neg}_{ij}$ paraphrases or unrelated edits \(x_l\) are pushed apart in the projection space. Training is performed by sampling pairs at random. Note that \(\vec{z}\) is a variable that is assigned either a paraphrase, an irrelevant prompt, or an unrelated edit just as a way to make the loss function more concise.</p> <p>The results presented in Figure 4 demonstrate that the projector network effectively learns to distance lexically similar but unrelated irrelevant prompts in comparison to paraphrases.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lexical_Dominance_projector-480.webp 480w,/assets/img/Lexical_Dominance_projector-800.webp 800w,/assets/img/Lexical_Dominance_projector-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Lexical_Dominance_projector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Projector networks mitigate lexical bias: a critical problem in adapter-based model editing techniques. Percentage of samples where irrelevant but lexically similar prompts are closer than semantically similar paraphrases in the representation space before and after our learned projection (PENME). </div> <h3 id="results">Results</h3> <p>We assess the performance of PENME across a spectrum of transformer-based LLMs, including T5, GPT2-XL and Llama-2-7b in the zsRE and Counterfact datasets. For comparitive performance to relevant literature please refer to the paper pdf at the bottom.</p> <table style="width:100%; border-collapse: collapse; text-align:center;"> <thead> <tr> <th rowspan="2">Method</th> <th rowspan="2">Model</th> <th colspan="4">Counterfact</th> <th colspan="4">zsRE</th> </tr> <tr> <th>ES</th> <th>Loc</th> <th>Para</th> <th>Score</th> <th>ES</th> <th>Loc</th> <th>Para</th> <th>Score</th> </tr> </thead> <tbody> <tr> <td>PENME</td> <td>T5-small</td> <td>1.000</td> <td>0.787</td> <td>0.808</td> <td>0.865</td> <td>1.000</td> <td>0.941</td> <td>0.913</td> <td>0.951</td> </tr> <tr> <td>PENME</td> <td>Llama-2-7b</td> <td>1.000</td> <td>0.869</td> <td>0.906</td> <td>0.925</td> <td>1.000</td> <td>0.987</td> <td>0.966</td> <td>0.984</td> </tr> <tr> <td>PENME</td> <td>GPT2-XL</td> <td>1.000</td> <td>0.847</td> <td>0.875</td> <td>0.907</td> <td>1.000</td> <td>0.957</td> <td>0.940</td> <td>0.966</td> </tr> <tr> <td>PENME<sub>stream</sub> </td> <td>T5-small</td> <td>1.000</td> <td>0.782</td> <td>0.756</td> <td>0.846</td> <td>1.000</td> <td>0.615</td> <td>0.550</td> <td>0.721</td> </tr> <tr> <td>PENME<sub>stream</sub> </td> <td>Llama-2-7b</td> <td>1.000</td> <td>0.871</td> <td>0.818</td> <td>0.896</td> <td>1.000</td> <td>0.716</td> <td>0.792</td> <td>0.836</td> </tr> <tr> <td>PENME<sub>stream</sub> </td> <td>GPT2-XL</td> <td>1.000</td> <td>0.850</td> <td>0.768</td> <td>0.872</td> <td>1.000</td> <td>0.733</td> <td>0.768</td> <td>0.833</td> </tr> </tbody> </table> <h3 id="finding-hyperparameter-and-scaling-edits">Finding Hyperparameter and Scaling Edits</h3> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/generalization_vs_locality_threshold_main-480.webp 480w,/assets/img/generalization_vs_locality_threshold_main-800.webp 800w,/assets/img/generalization_vs_locality_threshold_main-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/generalization_vs_locality_threshold_main.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5. Shows the trade-off between generalization and locality performance across different hyperparameter settings. The distance threshold τ varies from 0.01 to 0.2 (0.01 increments and τ is normalized by 100), while the edit-pairing similarity threshold ϕ ranges from 0.5 to 0.9 (0.1 increments). Higher ϕ values enforce stricter edit similarity requirements. The results showcase the effect of hyperparameter tuning on the projector network’s learning capacity and overall performance. </div> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/penme_ablation_samples-480.webp 480w,/assets/img/penme_ablation_samples-800.webp 800w,/assets/img/penme_ablation_samples-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/penme_ablation_samples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> This image can also have a caption. It's like magic. </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="Rizwan25Editing" class="col-sm-8"> <div class="title">Resolving Lexical Bias in Model Editing</div> <div class="author"> Hammad Rizwan, Domenic Rosati, Ga Wu, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hassan Sajjad' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2408.10411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hammad Rizwan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>